[{
    "id": "intelligent_rag_pipe",
    "user_id": "4fb3eabf-97d8-4d8f-af39-a618d9f22e16",
    "name": "Intelligent RAG",
    "type": "pipe",
    "content": "\"\"\"\ntitle: Intelligent RAG Function\nauthor: OpenHands\nauthor_url: https://github.com/41rumble/intelligent-rag-server\nversion: 0.1.0\n\nThis module defines a Pipe class that utilizes the intelligent-rag-server\n\"\"\"\n\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport time\nimport requests\nimport json\n\n\ndef extract_event_info(event_emitter) -> tuple[Optional[str], Optional[str]]:\n    if not event_emitter or not event_emitter.__closure__:\n        return None, None\n    for cell in event_emitter.__closure__:\n        if isinstance(request_info := cell.cell_contents, dict):\n            chat_id = request_info.get(\"chat_id\")\n            message_id = request_info.get(\"message_id\")\n            return chat_id, message_id\n    return None, None\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        server_url: str = Field(\n            default=\"http://localhost:3000\",\n            description=\"URL of the intelligent-rag-server\"\n        )\n        project_id: str = Field(\n            default=\"default\",\n            description=\"Project ID to query against\"\n        )\n        thinking_depth: int = Field(\n            default=2,\n            description=\"Depth of thinking (1-4). Higher values provide more detailed analysis\",\n            ge=1,\n            le=4\n        )\n        emit_interval: float = Field(\n            default=2.0,\n            description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True,\n            description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.type = \"pipe\"\n        self.id = \"intelligent_rag_pipe\"\n        self.name = \"Intelligent RAG\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool,\n    ):\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Optional[dict]:\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Querying intelligent RAG server...\", False\n        )\n        chat_id, _ = extract_event_info(__event_emitter__)\n        messages = body.get(\"messages\", [])\n\n        if messages:\n            question = messages[-1][\"content\"]\n            try:\n                headers = {\"Content-Type\": \"application/json\"}\n                payload = {\n                    \"projectId\": self.valves.project_id,\n                    \"query\": question,\n                    \"thinkingDepth\": self.valves.thinking_depth\n                }\n                response = requests.post(\n                    f\"{self.valves.server_url}/api/query\",\n                    json=payload,\n                    headers=headers\n                )\n                \n                if response.status_code == 200:\n                    rag_response = response.text\n                    \n                    # Extract the main answer and sources\n                    answer_text = \"\"\n                    sources = []\n                    thinking = []\n                    \n                    # Parse the response sections\n                    sections = rag_response.split(\"===\")\n                    for i, section in enumerate(sections):\n                        if i == 0: continue  # Skip empty first section\n                        \n                        section = section.strip()\n                        if section.startswith(\" Query Response \"):\n                            # Extract answer, removing the query line if present\n                            answer_lines = section.split(\"\\n\")[1:]  # Skip header\n                            if answer_lines[0].startswith(\"Query:\"):\n                                answer_lines = answer_lines[2:]  # Skip query and blank line\n                            answer_text = \"\\n\".join(answer_lines).strip()\n                            \n                        elif section.startswith(\" Source Snippets \"):\n                            # Parse source snippets\n                            source_blocks = section.split(\"[\")\n                            for block in source_blocks[1:]:  # Skip first empty block\n                                if \"]\" in block:\n                                    source_name, source_text = block.split(\"]\")\n                                    source_text = source_text.strip()\n                                    if \"Text:\" in source_text and \"Relevance:\" in source_text:\n                                        text = source_text.split(\"Text:\")[1].split(\"Relevance:\")[0].strip()\n                                        relevance = source_text.split(\"Relevance:\")[1].strip()\n                                        sources.append({\n                                            \"name\": source_name,\n                                            \"text\": text,\n                                            \"relevance\": relevance\n                                        })\n                        \n                        elif section.startswith(\" Thinking Process \"):\n                            # Extract thinking process\n                            thinking_lines = section.split(\"\\n\")[1:]  # Skip header\n                            thinking = [line.strip() for line in thinking_lines if line.strip()]\n                    \n                    # Format the final response in markdown\n                    final_answer = answer_text\n                    \n                    if thinking:\n                        final_answer += \"\\n\\nðŸ¤” **Thinking Process**\\n\" + \"\\n\".join(f\"- {thought}\" for thought in thinking)\n                    \n                    if sources:\n                        final_answer += \"\\n\\nðŸ“š **Sources**\\n\"\n                        for source in sources:\n                            final_answer += f\"- **[{source['name']}]**\\n\"\n                            final_answer += f\"  {source['text']}\\n\"\n                            final_answer += f\"  *Relevance: {source['relevance']}*\\n\\n\"\n                    \n                    # First emit the answer text\n                    await __event_emitter__({\n                        \"type\": \"message\",\n                        \"data\": {\n                            \"role\": \"assistant\",\n                            \"content\": final_answer\n                        }\n                    })\n                    \n                    # Then emit completion\n                    await __event_emitter__({\n                        \"type\": \"message\",\n                        \"data\": {\n                            \"done\": True\n                        }\n                    })\n                    \n                    # Return empty to signal we handled the streaming\n                    return None\n                    \n                else:\n                    raise Exception(f\"Error: {response.status_code} - {response.text}\")\n\n            except Exception as e:\n                await self.emit_status(\n                    __event_emitter__,\n                    \"error\",\n                    f\"Error during RAG query: {str(e)}\",\n                    True,\n                )\n                return {\"error\": str(e)}\n        else:\n            await self.emit_status(\n                __event_emitter__,\n                \"error\",\n                \"No messages found in the request body\",\n                True,\n            )\n            await __event_emitter__({\n                \"type\": \"message\",\n                \"data\": {\n                    \"role\": \"assistant\",\n                    \"content\": \"No messages found in the request body\"\n                }\n            })\n            await __event_emitter__({\n                \"type\": \"message\",\n                \"data\": {\n                    \"done\": True\n                }\n            })\n            return None\n\n        await self.emit_status(__event_emitter__, \"info\", \"Complete\", True)\n        return None",
    "meta": {
        "description": "Allows you to chat with an intelligent RAG server that provides enhanced search and reasoning capabilities",
        "manifest": {
            "title": "Intelligent RAG Function",
            "author": "OpenHands",
            "author_url": "https://github.com/41rumble/intelligent-rag-server",
            "version": "0.1.0"
        }
    },
    "is_active": false,
    "is_global": false,
    "updated_at": 1747598592,
    "created_at": 1747598592
}]