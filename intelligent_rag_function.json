[{
    "id": "intelligent_rag_pipe",
    "user_id": "4fb3eabf-97d8-4d8f-af39-a618d9f22e16",
    "name": "Intelligent RAG",
    "type": "pipe",
    "content": "\"\"\"\ntitle: Intelligent RAG Function\nauthor: OpenHands\nauthor_url: https://github.com/41rumble/intelligent-rag-server\nversion: 0.1.0\n\nThis module defines a Pipe class that utilizes the intelligent-rag-server\n\"\"\"\n\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport time\nimport requests\nimport json\n\n\ndef extract_event_info(event_emitter) -> tuple[Optional[str], Optional[str]]:\n    if not event_emitter or not event_emitter.__closure__:\n        return None, None\n    for cell in event_emitter.__closure__:\n        if isinstance(request_info := cell.cell_contents, dict):\n            chat_id = request_info.get(\"chat_id\")\n            message_id = request_info.get(\"message_id\")\n            return chat_id, message_id\n    return None, None\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        server_url: str = Field(\n            default=\"http://localhost:3001\",\n            description=\"URL of the intelligent-rag-server\"\n        )\n        project_id: str = Field(\n            default=\"the_great_fire\",\n            description=\"Project ID to query against\"\n        )\n        thinking_depth: int = Field(\n            default=7,\n            description=\"Depth of thinking (1-10). Higher values mean more thorough but slower responses.\",\n            ge=1,\n            le=10,\n            chat_control=True  # Enable real-time control in chat\n        )\n        emit_interval: float = Field(\n            default=2.0,\n            description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True,\n            description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.type = \"pipe\"\n        self.id = \"intelligent_rag_pipe\"\n        self.name = \"Intelligent RAG\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n        self.citation = False  # Prevent automatic citation generation\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool,\n    ):\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Optional[dict]:\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Querying intelligent RAG server...\", False\n        )\n        chat_id, _ = extract_event_info(__event_emitter__)\n        messages = body.get(\"messages\", [])\n\n        if messages:\n            question = messages[-1][\"content\"]\n            try:\n                headers = {\"Content-Type\": \"application/json\"}\n                payload = {\n                    \"projectId\": self.valves.project_id,\n                    \"query\": question,\n                    \"thinkingDepth\": self.valves.thinking_depth\n                }\n                with requests.post(\n                    f\"{self.valves.server_url}/api/query\",\n                    json=payload,\n                    headers=headers,\n                    stream=True\n                ) as response:\n                    if response.status_code == 200:\n                        answer = None\n                        buffer = \"\"\n                        source_snippets = []\n                        log = \"\"\n                        answer_text = \"\"\n                        \n                        # Process the streaming response\n                        for chunk in response.iter_lines(decode_unicode=True):\n                            if chunk:  # Filter out keep-alive new lines\n                                try:\n                                    update = json.loads(chunk)\n                                    \n                                    # Handle progress updates\n                                    if update.get('type') == 'progress':\n                                        progress = update['data']\n                                        current_step = progress['current_step']\n                                        completed = progress['completed_steps']\n                                        total = progress['total_steps']\n                                        step_info = next(\n                                            (s for s in progress['steps'] if s['id'] == current_step),\n                                            None\n                                        )\n                                        \n                                        if step_info:\n                                            await self.emit_status(\n                                                __event_emitter__,\n                                                \"info\",\n                                                f\"{step_info['message']} ({completed}/{total})\\n{step_info['details']}\",\n                                                False\n                                            )\n                                    \n                                    # Handle answer progress\n                                    elif update.get('type') == 'answer_progress':\n                                        data = update['data']\n                                        \n                                        # If this is a chunk of the answer\n                                        if not data.get('complete'):\n                                            chunk_text = data.get('text', '')\n                                            answer_text += chunk_text\n                                            \n                                            # Emit the chunk as it comes\n                                            if chunk_text:\n                                                await __event_emitter__({\n                                                    \"type\": \"partial_response\",\n                                                    \"data\": {\n                                                        \"text\": chunk_text\n                                                    }\n                                                })\n                                        \n                                        # If this is the final answer\n                                        else:\n                                            answer = data.get('answer', '')\n                                            source_snippets = data.get('source_snippets', [])\n                                            log = data.get('log', {})\n                                except json.JSONDecodeError:\n                                    print(f\"Failed to parse JSON: {chunk}\")\n                                    continue\n                        \n                        # Emit citations for source snippets\n                        if source_snippets and __event_emitter__:\n                            for snippet in source_snippets:\n                                await __event_emitter__({\n                                    \"type\": \"citation\",\n                                    \"data\": {\n                                        \"document\": [snippet[\"text\"]],\n                                        \"metadata\": [{\"source\": snippet[\"id\"]}],\n                                        \"source\": {\"name\": snippet[\"id\"]}\n                                    }\n                                })\n                        \n                        # Add thinking process if available\n                        if log:\n                            body[\"messages\"].append({\n                                \"role\": \"system\",\n                                \"content\": f\"Thinking process:\\n{json.dumps(log, indent=2)}\"\n                            })\n                        \n                        # Add final answer\n                        body[\"messages\"].append({\n                            \"role\": \"assistant\",\n                            \"content\": answer or answer_text\n                        })\n                        \n                        return answer or answer_text\n                    else:\n                        raise Exception(f\"Error: {response.status_code} - {response.text}\")\n\n            except Exception as e:\n                await self.emit_status(\n                    __event_emitter__,\n                    \"error\",\n                    f\"Error during RAG query: {str(e)}\",\n                    True,\n                )\n                return {\"error\": str(e)}\n        else:\n            await self.emit_status(\n                __event_emitter__,\n                \"error\",\n                \"No messages found in the request body\",\n                True,\n            )\n            body[\"messages\"].append({\n                \"role\": \"assistant\",\n                \"content\": \"No messages found in the request body\"\n            })\n\n        await self.emit_status(__event_emitter__, \"info\", \"Complete\", True)\n        return answer",
    "meta": {
        "description": "Intelligent RAG function for enhanced search and reasoning",
        "input_schema": {
            "type": "object",
            "properties": {
                "messages": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "role": {
                                "type": "string",
                                "enum": ["user", "assistant", "system"]
                            },
                            "content": {
                                "type": "string"
                            }
                        },
                        "required": ["role", "content"]
                    }
                }
            },
            "required": ["messages"]
        },
        "output_schema": {
            "type": "object",
            "properties": {
                "answer": {
                    "type": "string"
                },
                "source_snippets": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {
                                "type": "string"
                            },
                            "text": {
                                "type": "string"
                            },
                            "source": {
                                "type": "string"
                            }
                        }
                    }
                },
                "log": {
                    "type": "object"
                }
            }
        }
    }
}]