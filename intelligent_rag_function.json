[{
    "id": "intelligent_rag_pipe",
    "user_id": "4fb3eabf-97d8-4d8f-af39-a618d9f22e16",
    "name": "Intelligent RAG",
    "type": "pipe",
    "content": "\"\"\"\ntitle: Intelligent RAG Function\nauthor: OpenHands\nauthor_url: https://github.com/41rumble/intelligent-rag-server\nversion: 0.1.0\n\nThis module defines a Pipe class that utilizes the intelligent-rag-server\n\"\"\"\n\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport time\nimport requests\nimport json\n\n\ndef extract_event_info(event_emitter) -> tuple[Optional[str], Optional[str]]:\n    if not event_emitter or not event_emitter.__closure__:\n        return None, None\n    for cell in event_emitter.__closure__:\n        if isinstance(request_info := cell.cell_contents, dict):\n            chat_id = request_info.get(\"chat_id\")\n            message_id = request_info.get(\"message_id\")\n            return chat_id, message_id\n    return None, None\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        server_url: str = Field(\n            default=\"http://localhost:3000\",\n            description=\"URL of the intelligent-rag-server\"\n        )\n        project_id: str = Field(\n            default=\"default\",\n            description=\"Project ID to query against\"\n        )\n        thinking_depth: int = Field(\n            default=2,\n            description=\"Depth of thinking (1-4). Higher values provide more detailed analysis\",\n            ge=1,\n            le=4\n        )\n        emit_interval: float = Field(\n            default=2.0,\n            description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True,\n            description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.type = \"pipe\"\n        self.id = \"intelligent_rag_pipe\"\n        self.name = \"Intelligent RAG\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool,\n    ):\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Optional[dict]:\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Querying intelligent RAG server...\", False\n        )\n        chat_id, _ = extract_event_info(__event_emitter__)\n        messages = body.get(\"messages\", [])\n\n        if messages:\n            question = messages[-1][\"content\"]\n            try:\n                headers = {\"Content-Type\": \"application/json\"}\n                payload = {\n                    \"projectId\": self.valves.project_id,\n                    \"query\": question,\n                    \"thinkingDepth\": self.valves.thinking_depth\n                }\n                response = requests.post(\n                    f\"{self.valves.server_url}/api/query\",\n                    json=payload,\n                    headers=headers\n                )\n                \n                if response.status_code == 200:\n                    rag_response = response.json()\n                    \n                    # Extract the main answer and sources\n                    answer_text = \"\"\n                    sources = []\n                    \n                    # Check for === Query Response === section\n                    if \"=== Query Response ===\" in rag_response:\n                        parts = rag_response.split(\"=== Query Response ===\")\n                        if len(parts) > 1:\n                            answer_part = parts[1].split(\"=== Source Snippets ===\")[0].strip()\n                            if answer_part.startswith(\"Query:\"):  # Remove the query line if present\n                                answer_text = \"\\n\".join(answer_part.split(\"\\n\")[2:]).strip()\n                            else:\n                                answer_text = answer_part\n                    else:\n                        answer_text = rag_response.get(\"answer\", \"\")\n                    \n                    # Check for === Source Snippets === section\n                    if \"=== Source Snippets ===\" in rag_response:\n                        sources_part = rag_response.split(\"=== Source Snippets ===\")[1].strip()\n                        # Parse source snippets - they're in blocks with [source_name] headers\n                        source_blocks = sources_part.split(\"[\")\n                        for block in source_blocks:\n                            if block.strip():\n                                # Extract source name and text\n                                if \"]\" in block:\n                                    source_name, source_text = block.split(\"]\")\n                                    source_text = source_text.strip()\n                                    if \"Text:\" in source_text and \"Relevance:\" in source_text:\n                                        text_part = source_text.split(\"Text:\")[1].split(\"Relevance:\")[0].strip()\n                                        relevance = source_text.split(\"Relevance:\")[1].strip()\n                                        sources.append(f\"[{source_name}] {text_part}\\nRelevance: {relevance}\")\n                    \n                    # Build the final answer with sources\n                    final_answer = answer_text\n                    if sources:\n                        final_answer += \"\\n\\nðŸ“š Sources:\\n\" + \"\\n\\n\".join(f\"- {source}\" for source in sources)\n                    \n                    body[\"messages\"].append({\n                        \"role\": \"assistant\",\n                        \"content\": final_answer\n                    })\n                    \n                else:\n                    raise Exception(f\"Error: {response.status_code} - {response.text}\")\n\n            except Exception as e:\n                await self.emit_status(\n                    __event_emitter__,\n                    \"error\",\n                    f\"Error during RAG query: {str(e)}\",\n                    True,\n                )\n                return {\"error\": str(e)}\n        else:\n            await self.emit_status(\n                __event_emitter__,\n                \"error\",\n                \"No messages found in the request body\",\n                True,\n            )\n            body[\"messages\"].append({\n                \"role\": \"assistant\",\n                \"content\": \"No messages found in the request body\"\n            })\n\n        await self.emit_status(__event_emitter__, \"info\", \"Complete\", True)\n        return answer_text",
    "meta": {
        "description": "Allows you to chat with an intelligent RAG server that provides enhanced search and reasoning capabilities",
        "manifest": {
            "title": "Intelligent RAG Function",
            "author": "OpenHands",
            "author_url": "https://github.com/41rumble/intelligent-rag-server",
            "version": "0.1.0"
        }
    },
    "is_active": false,
    "is_global": false,
    "updated_at": 1747598592,
    "created_at": 1747598592
}]