[{
    "id": "intelligent_rag_pipe",
    "user_id": "4fb3eabf-97d8-4d8f-af39-a618d9f22e16",
    "name": "The Great Fire: Researcher",
    "type": "pipe",
    "content": "\"\"\"\ntitle: Intelligent RAG Function\nauthor: OpenHands\nauthor_url: https://github.com/41rumble/intelligent-rag-server\nversion: 0.1.0\n\nThis module defines a Pipe class that utilizes the intelligent-rag-server\n\"\"\"\n\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport time\nimport requests\nimport json\n\n\ndef extract_event_info(event_emitter) -> tuple[Optional[str], Optional[str]]:\n    if not event_emitter or not event_emitter.__closure__:\n        return None, None\n    for cell in event_emitter.__closure__:\n        if isinstance(request_info := cell.cell_contents, dict):\n            chat_id = request_info.get(\"chat_id\")\n            message_id = request_info.get(\"message_id\")\n            return chat_id, message_id\n    return None, None\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        server_url: str = Field(\n            default=\"http://localhost:3001\",\n            description=\"URL of the intelligent-rag-server\"\n        )\n        project_id: str = Field(\n            default=\"the_great_fire\",\n            description=\"Project ID to query against\"\n        )\n        thinking_depth: int = Field(\n            default=7,\n            description=\"Depth of thinking (1-10). Higher values mean more thorough but slower responses.\",\n            ge=1,\n            le=10,\n            chat_control=True  # Enable real-time control in chat\n        )\n        emit_interval: float = Field(\n            default=2.0,\n            description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True,\n            description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.type = \"pipe\"\n        self.id = \"intelligent_rag_pipe\"\n        self.name = \"The Great Fire: Researcher\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n        self.citation = False  # Prevent automatic citation generation\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool,\n    ):\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Optional[dict]:\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Having a good honest think about it...\", False\n        )\n        chat_id, _ = extract_event_info(__event_emitter__)\n        messages = body.get(\"messages\", [])\n\n        if messages:\n            question = messages[-1][\"content\"]\n            try:\n                headers = {\"Content-Type\": \"application/json\"}\n                payload = {\n                    \"projectId\": self.valves.project_id,\n                    \"query\": question,\n                    \"thinkingDepth\": self.valves.thinking_depth\n                }\n                response = requests.post(\n                    f\"{self.valves.server_url}/api/query\",\n                    json=payload,\n                    headers=headers\n                )\n                \n                if response.status_code == 200:\n                    rag_response = response.json()\n                    answer = rag_response.get(\"answer\", \"\")\n                    log = rag_response.get(\"log\", \"\")\n                    source_snippets = rag_response.get(\"source_snippets\", [])\n                    \n                    # Emit citations for source snippets\n                    if source_snippets and __event_emitter__:\n                        for snippet in source_snippets:\n                            # Use title if available, fallback to id\n                            citation_title = snippet.get(\"title\", snippet[\"id\"])\n                            await __event_emitter__({\n                                \"type\": \"citation\",\n                                \"data\": {\n                                    \"document\": [snippet[\"text\"]],\n                                    \"metadata\": [{\"source\": snippet[\"id\"]}],\n                                    \"source\": {\"name\": citation_title}\n                                }\n                            })\n                    \n                    if log:\n                        body[\"messages\"].append({\n                            \"role\": \"system\",\n                            \"content\": f\"Thinking process:\\n{log}\"\n                        })\n                    \n                    body[\"messages\"].append({\n                        \"role\": \"assistant\",\n                        \"content\": answer\n                    })\n                    \n                    # Emit completion status after answer is added\n                    await self.emit_status(\n                        __event_emitter__,\n                        \"info\",\n                        \"Research complete\",\n                        True\n                    )\n                    \n                else:\n                    raise Exception(f\"Error: {response.status_code} - {response.text}\")\n\n            except Exception as e:\n                await self.emit_status(\n                    __event_emitter__,\n                    \"error\",\n                    f\"Error during RAG query: {str(e)}\",\n                    True,\n                )\n                return {\"error\": str(e)}\n        else:\n            await self.emit_status(\n                __event_emitter__,\n                \"error\",\n                \"No messages found in the request body\",\n                True,\n            )\n            body[\"messages\"].append({\n                \"role\": \"assistant\",\n                \"content\": \"No messages found in the request body\"\n            })\n\n        return answer",
    "meta": {
        "description": "Allows you to chat with an intelligent RAG server that provides enhanced search and reasoning capabilities",
        "manifest": {
            "title": "The Great Fire: Researcher",
            "author": "OpenHands",
            "author_url": "https://github.com/41rumble/intelligent-rag-server",
            "version": "0.1.0"
        }
    },
    "is_active": false,
    "is_global": false,
    "updated_at": 1747598592,
    "created_at": 1747598592
}]