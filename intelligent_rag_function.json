[{
    "id": "intelligent_rag_pipe",
    "user_id": "4fb3eabf-97d8-4d8f-af39-a618d9f22e16",
    "name": "Intelligent RAG",
    "type": "pipe",
    "content": "\"\"\"\ntitle: Intelligent RAG Function\nauthor: OpenHands\nauthor_url: https://github.com/41rumble/intelligent-rag-server\nversion: 0.1.0\n\nThis module defines a Pipe class that utilizes the intelligent-rag-server\n\"\"\"\n\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport time\nimport requests\nimport json\n\n\ndef extract_event_info(event_emitter) -> tuple[Optional[str], Optional[str]]:\n    if not event_emitter or not event_emitter.__closure__:\n        return None, None\n    for cell in event_emitter.__closure__:\n        if isinstance(request_info := cell.cell_contents, dict):\n            chat_id = request_info.get(\"chat_id\")\n            message_id = request_info.get(\"message_id\")\n            return chat_id, message_id\n    return None, None\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        server_url: str = Field(\n            default=\"http://localhost:3001\",\n            description=\"URL of the intelligent-rag-server\"\n        )\n        project_id: str = Field(\n            default=\"the_great_fire\",\n            description=\"Project ID to query against\"\n        )\n        thinking_depth: int = Field(\n            default=7,\n            description=\"Depth of thinking (1-10). Higher values mean more thorough but slower responses.\",\n            ge=1,\n            le=10,\n            chat_control=True  # Enable real-time control in chat\n        )\n        emit_interval: float = Field(\n            default=2.0,\n            description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True,\n            description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.type = \"pipe\"\n        self.id = \"intelligent_rag_pipe\"\n        self.name = \"Intelligent RAG\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n        self.citation = False  # Prevent automatic citation generation\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool,\n    ):\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Optional[dict]:\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Querying intelligent RAG server...\", False\n        )\n        chat_id, _ = extract_event_info(__event_emitter__)\n        messages = body.get(\"messages\", [])\n\n        if not messages:\n            await self.emit_status(\n                __event_emitter__,\n                \"error\",\n                \"No messages found in the request body\",\n                True,\n            )\n            body[\"messages\"].append({\n                \"role\": \"assistant\",\n                \"content\": \"No messages found in the request body\"\n            })\n            return None\n\n        question = messages[-1][\"content\"]\n        try:\n            headers = {\"Content-Type\": \"application/json\"}\n            payload = {\n                \"projectId\": self.valves.project_id,\n                \"query\": question,\n                \"thinkingDepth\": self.valves.thinking_depth\n            }\n            with requests.post(\n                f\"{self.valves.server_url}/api/query\",\n                json=payload,\n                headers=headers,\n                stream=True\n            ) as response:\n                if response.status_code != 200:\n                    raise Exception(f\"Error: {response.status_code} - {response.text}\")\n                    \n                answer = None\n                buffer = \"\"\n                source_snippets = []\n                log = \"\"\n                answer_text = \"\"\n                citations_sent = False\n                \n                # Process the streaming response\n                for chunk in response.iter_lines(decode_unicode=True):\n                    if not chunk:  # Skip empty lines\n                        continue\n                        \n                    try:\n                        update = json.loads(chunk)\n                        \n                        # Handle progress updates\n                        if update.get('type') == 'response':\n                            data = update['data']\n                            progress = data.get('progress', {})\n                            \n                            # Handle progress info\n                            if progress:\n                                current_step = progress.get('current_step')\n                                completed = progress.get('completed_steps', 0)\n                                total = progress.get('total_steps', 5)\n                                step_info = next(\n                                    (s for s in progress.get('steps', []) if s['id'] == current_step),\n                                    None\n                                )\n                                \n                                if step_info:\n                                    await self.emit_status(\n                                        __event_emitter__,\n                                        \"info\",\n                                        f\"{step_info['message']} ({completed}/{total})\\n{step_info['details']}\",\n                                        current_step == 'completed'\n                                    )\n                            \n                            # Send citations only once when we first get them\n                            if not citations_sent and data.get('source_snippets'):\n                                source_snippets = data['source_snippets']\n                                if source_snippets and __event_emitter__:\n                                    for snippet in source_snippets:\n                                        await __event_emitter__({\n                                            \"type\": \"citation\",\n                                            \"data\": {\n                                                \"document\": [snippet[\"text\"]],\n                                                \"metadata\": [{\"source\": snippet[\"id\"]}],\n                                                \"source\": {\"name\": snippet[\"id\"]}\n                                            }\n                                        })\n                                citations_sent = True\n                            \n                            # If this is a chunk of the answer\n                            if not data.get('complete'):\n                                chunk_text = data.get('text', '')\n                                if chunk_text:\n                                    answer_text += chunk_text\n                                    await __event_emitter__({\n                                        \"type\": \"partial_response\",\n                                        \"data\": {\n                                            \"text\": chunk_text\n                                        }\n                                    })\n                            \n                            # If this is the final answer\n                            elif data.get('complete'):\n                                answer = data.get('answer', '')\n                                log = data.get('log', {})\n                                \n                                # Add thinking process if available\n                                if log:\n                                    body[\"messages\"].append({\n                                        \"role\": \"system\",\n                                        \"content\": f\"Thinking process:\\n{json.dumps(log, indent=2)}\"\n                                    })\n                                \n                                # Add final answer and return\n                                final_answer = answer or answer_text\n                                body[\"messages\"].append({\n                                    \"role\": \"assistant\",\n                                    \"content\": final_answer\n                                })\n                                return final_answer\n                                \n                    except json.JSONDecodeError:\n                        print(f\"Failed to parse JSON: {chunk}\")\n                        continue\n\n        except Exception as e:\n            await self.emit_status(\n                __event_emitter__,\n                \"error\",\n                f\"Error during RAG query: {str(e)}\",\n                True,\n            )\n            return {\"error\": str(e)}\n\n        # Should never reach here due to return in complete block\n        return None",
    "meta": {
        "description": "Intelligent RAG function for enhanced search and reasoning",
        "input_schema": {
            "type": "object",
            "properties": {
                "messages": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "role": {
                                "type": "string",
                                "enum": ["user", "assistant", "system"]
                            },
                            "content": {
                                "type": "string"
                            }
                        },
                        "required": ["role", "content"]
                    }
                }
            },
            "required": ["messages"]
        },
        "output_schema": {
            "type": "object",
            "properties": {
                "answer": {
                    "type": "string"
                },
                "source_snippets": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {
                                "type": "string"
                            },
                            "text": {
                                "type": "string"
                            },
                            "source": {
                                "type": "string"
                            }
                        }
                    }
                },
                "log": {
                    "type": "object"
                }
            }
        }
    }
}]